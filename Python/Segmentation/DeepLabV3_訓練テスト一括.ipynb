{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7901c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library import successful.\n",
      "IMG_SIZE: (64, 64), batch_size: 2, classes: ['background', 'debris'], num_classes: 2 \n",
      "num_epochs: 200, valsplit: 0.25, learning_rate: 0.0001 \n",
      "loss_type: ce, optimizer_type: adam, DEVICE: cuda \n",
      "\n",
      "Started at 20260103_18:10:25\n",
      "TrainData Size: 6, TrainData Batches: 3 \n",
      "ValData Size: 2, ValData Batches: 1\n",
      "TestData Size: 2, TestData Batches: 1\n",
      "Class counts: [21940  2636], freq: [0.89274089 0.10725911], weights: tensor([0.5601, 4.6616], device='cuda:0')\n",
      "-----TRAINING PHASE-----\n",
      "Epoch 1: train_loss=0.7117, train_mIoU=0.0642, val_loss=0.5214, val_mIoU=0.1738\n",
      "Epoch 2: train_loss=0.6909, train_mIoU=0.1547, val_loss=0.5211, val_mIoU=0.1738\n",
      "Epoch 3: train_loss=0.6175, train_mIoU=0.1329, val_loss=0.5066, val_mIoU=0.1738\n",
      "Epoch 4: train_loss=0.5948, train_mIoU=0.2046, val_loss=0.5394, val_mIoU=0.1816\n",
      "Epoch 5: train_loss=0.5467, train_mIoU=0.2904, val_loss=0.5362, val_mIoU=0.2156\n",
      "Epoch 6: train_loss=0.5144, train_mIoU=0.3674, val_loss=0.5083, val_mIoU=0.1738\n",
      "Epoch 7: train_loss=0.4522, train_mIoU=0.4391, val_loss=0.5435, val_mIoU=0.2122\n",
      "Epoch 8: train_loss=0.4252, train_mIoU=0.4569, val_loss=0.5579, val_mIoU=0.2438\n",
      "Epoch 9: train_loss=0.3993, train_mIoU=0.4943, val_loss=0.5646, val_mIoU=0.1930\n",
      "Epoch 10: train_loss=0.3682, train_mIoU=0.5625, val_loss=0.5689, val_mIoU=0.2507\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch010.pth\n",
      "Epoch 11: train_loss=0.3493, train_mIoU=0.5939, val_loss=0.5898, val_mIoU=0.3466\n",
      "Epoch 12: train_loss=0.3415, train_mIoU=0.5663, val_loss=0.6602, val_mIoU=0.3515\n",
      "Epoch 13: train_loss=0.3140, train_mIoU=0.6385, val_loss=0.6550, val_mIoU=0.3323\n",
      "Epoch 14: train_loss=0.2948, train_mIoU=0.6481, val_loss=0.6479, val_mIoU=0.3853\n",
      "Epoch 15: train_loss=0.2802, train_mIoU=0.6475, val_loss=0.7386, val_mIoU=0.4091\n",
      "Epoch 16: train_loss=0.2663, train_mIoU=0.6760, val_loss=0.7513, val_mIoU=0.3986\n",
      "Epoch 17: train_loss=0.2702, train_mIoU=0.6903, val_loss=0.8064, val_mIoU=0.4079\n",
      "Epoch 18: train_loss=0.2460, train_mIoU=0.6931, val_loss=0.7768, val_mIoU=0.3799\n",
      "Epoch 19: train_loss=0.2325, train_mIoU=0.7249, val_loss=0.8263, val_mIoU=0.3879\n",
      "Epoch 20: train_loss=0.2231, train_mIoU=0.6984, val_loss=0.9076, val_mIoU=0.3539\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch020.pth\n",
      "Epoch 21: train_loss=0.2362, train_mIoU=0.7059, val_loss=0.9524, val_mIoU=0.3134\n",
      "Epoch 22: train_loss=0.2076, train_mIoU=0.7291, val_loss=0.9911, val_mIoU=0.3090\n",
      "Epoch 23: train_loss=0.1959, train_mIoU=0.7459, val_loss=1.0239, val_mIoU=0.3128\n",
      "Epoch 24: train_loss=0.1917, train_mIoU=0.7579, val_loss=0.9959, val_mIoU=0.3177\n",
      "Epoch 25: train_loss=0.1845, train_mIoU=0.7707, val_loss=1.0446, val_mIoU=0.3126\n",
      "Epoch 26: train_loss=0.1923, train_mIoU=0.7483, val_loss=1.1555, val_mIoU=0.3098\n",
      "Epoch 27: train_loss=0.1802, train_mIoU=0.7646, val_loss=1.1601, val_mIoU=0.3032\n",
      "Epoch 28: train_loss=0.1884, train_mIoU=0.7566, val_loss=1.2128, val_mIoU=0.3132\n",
      "Epoch 29: train_loss=0.1650, train_mIoU=0.7811, val_loss=1.1678, val_mIoU=0.3129\n",
      "Epoch 30: train_loss=0.1655, train_mIoU=0.7718, val_loss=1.2207, val_mIoU=0.3146\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch030.pth\n",
      "Epoch 31: train_loss=0.1576, train_mIoU=0.7832, val_loss=1.2770, val_mIoU=0.3246\n",
      "Epoch 32: train_loss=0.1711, train_mIoU=0.7905, val_loss=1.3226, val_mIoU=0.3262\n",
      "Epoch 33: train_loss=0.1542, train_mIoU=0.7880, val_loss=1.3593, val_mIoU=0.3262\n",
      "Epoch 34: train_loss=0.1519, train_mIoU=0.7925, val_loss=1.3193, val_mIoU=0.3247\n",
      "Epoch 35: train_loss=0.1602, train_mIoU=0.8047, val_loss=1.3062, val_mIoU=0.3262\n",
      "Epoch 36: train_loss=0.1520, train_mIoU=0.7914, val_loss=1.3736, val_mIoU=0.3262\n",
      "Epoch 37: train_loss=0.1520, train_mIoU=0.8024, val_loss=1.2231, val_mIoU=0.3120\n",
      "Epoch 38: train_loss=0.1468, train_mIoU=0.8086, val_loss=1.2666, val_mIoU=0.3206\n",
      "Epoch 39: train_loss=0.1464, train_mIoU=0.8174, val_loss=1.2915, val_mIoU=0.3232\n",
      "Epoch 40: train_loss=0.1358, train_mIoU=0.8020, val_loss=1.3308, val_mIoU=0.3252\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch040.pth\n",
      "Epoch 41: train_loss=0.1342, train_mIoU=0.7986, val_loss=1.2803, val_mIoU=0.3232\n",
      "Epoch 42: train_loss=0.1312, train_mIoU=0.7985, val_loss=1.3477, val_mIoU=0.3245\n",
      "Epoch 43: train_loss=0.1399, train_mIoU=0.8207, val_loss=1.4915, val_mIoU=0.3262\n",
      "Epoch 44: train_loss=0.1318, train_mIoU=0.8315, val_loss=1.5069, val_mIoU=0.3262\n",
      "Epoch 45: train_loss=0.1337, train_mIoU=0.8273, val_loss=1.4329, val_mIoU=0.3261\n",
      "Epoch 46: train_loss=0.1286, train_mIoU=0.8339, val_loss=1.4778, val_mIoU=0.3262\n",
      "Epoch 47: train_loss=0.1202, train_mIoU=0.8298, val_loss=1.5491, val_mIoU=0.3262\n",
      "Epoch 48: train_loss=0.1257, train_mIoU=0.8326, val_loss=1.5048, val_mIoU=0.3262\n",
      "Epoch 49: train_loss=0.1310, train_mIoU=0.8414, val_loss=1.5339, val_mIoU=0.3262\n",
      "Epoch 50: train_loss=0.1161, train_mIoU=0.8415, val_loss=1.5354, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch050.pth\n",
      "Epoch 51: train_loss=0.1161, train_mIoU=0.8347, val_loss=1.5815, val_mIoU=0.3262\n",
      "Epoch 52: train_loss=0.1210, train_mIoU=0.8411, val_loss=1.6106, val_mIoU=0.3262\n",
      "Epoch 53: train_loss=0.1171, train_mIoU=0.8346, val_loss=1.6480, val_mIoU=0.3262\n",
      "Epoch 54: train_loss=0.1149, train_mIoU=0.8455, val_loss=1.6373, val_mIoU=0.3262\n",
      "Epoch 55: train_loss=0.1148, train_mIoU=0.8404, val_loss=1.5551, val_mIoU=0.3259\n",
      "Epoch 56: train_loss=0.1116, train_mIoU=0.8425, val_loss=1.5759, val_mIoU=0.3262\n",
      "Epoch 57: train_loss=0.1107, train_mIoU=0.8496, val_loss=1.4936, val_mIoU=0.3262\n",
      "Epoch 58: train_loss=0.1106, train_mIoU=0.8334, val_loss=1.6221, val_mIoU=0.3262\n",
      "Epoch 59: train_loss=0.1085, train_mIoU=0.8543, val_loss=1.6490, val_mIoU=0.3262\n",
      "Epoch 60: train_loss=0.1067, train_mIoU=0.8511, val_loss=1.7058, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch060.pth\n",
      "Epoch 61: train_loss=0.1017, train_mIoU=0.8594, val_loss=1.7452, val_mIoU=0.3262\n",
      "Epoch 62: train_loss=0.1001, train_mIoU=0.8554, val_loss=1.7439, val_mIoU=0.3262\n",
      "Epoch 63: train_loss=0.1051, train_mIoU=0.8535, val_loss=1.7880, val_mIoU=0.3262\n",
      "Epoch 64: train_loss=0.1021, train_mIoU=0.8627, val_loss=1.6924, val_mIoU=0.3262\n",
      "Epoch 65: train_loss=0.0968, train_mIoU=0.8503, val_loss=1.7086, val_mIoU=0.3262\n",
      "Epoch 66: train_loss=0.1028, train_mIoU=0.8507, val_loss=1.8070, val_mIoU=0.3262\n",
      "Epoch 67: train_loss=0.0976, train_mIoU=0.8673, val_loss=1.7848, val_mIoU=0.3262\n",
      "Epoch 68: train_loss=0.0975, train_mIoU=0.8615, val_loss=1.7809, val_mIoU=0.3262\n",
      "Epoch 69: train_loss=0.0968, train_mIoU=0.8599, val_loss=1.7387, val_mIoU=0.3262\n",
      "Epoch 70: train_loss=0.0924, train_mIoU=0.8600, val_loss=1.8203, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch070.pth\n",
      "Epoch 71: train_loss=0.0934, train_mIoU=0.8555, val_loss=1.8203, val_mIoU=0.3262\n",
      "Epoch 72: train_loss=0.0954, train_mIoU=0.8536, val_loss=1.9627, val_mIoU=0.3262\n",
      "Epoch 73: train_loss=0.0913, train_mIoU=0.8620, val_loss=1.9117, val_mIoU=0.3262\n",
      "Epoch 74: train_loss=0.0932, train_mIoU=0.8678, val_loss=1.9096, val_mIoU=0.3262\n",
      "Epoch 75: train_loss=0.0916, train_mIoU=0.8703, val_loss=1.9170, val_mIoU=0.3262\n",
      "Epoch 76: train_loss=0.0880, train_mIoU=0.8699, val_loss=1.8878, val_mIoU=0.3262\n",
      "Epoch 77: train_loss=0.0864, train_mIoU=0.8653, val_loss=1.8546, val_mIoU=0.3262\n",
      "Epoch 78: train_loss=0.0901, train_mIoU=0.8607, val_loss=1.8026, val_mIoU=0.3262\n",
      "Epoch 79: train_loss=0.0873, train_mIoU=0.8650, val_loss=1.8692, val_mIoU=0.3262\n",
      "Epoch 80: train_loss=0.0890, train_mIoU=0.8706, val_loss=1.8243, val_mIoU=0.3260\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch080.pth\n",
      "Epoch 81: train_loss=0.0875, train_mIoU=0.8660, val_loss=1.8494, val_mIoU=0.3262\n",
      "Epoch 82: train_loss=0.0841, train_mIoU=0.8711, val_loss=1.9060, val_mIoU=0.3262\n",
      "Epoch 83: train_loss=0.0840, train_mIoU=0.8647, val_loss=1.8925, val_mIoU=0.3262\n",
      "Epoch 84: train_loss=0.0845, train_mIoU=0.8738, val_loss=1.9060, val_mIoU=0.3262\n",
      "Epoch 85: train_loss=0.0846, train_mIoU=0.8737, val_loss=1.8399, val_mIoU=0.3262\n",
      "Epoch 86: train_loss=0.0864, train_mIoU=0.8656, val_loss=1.8923, val_mIoU=0.3259\n",
      "Epoch 87: train_loss=0.0823, train_mIoU=0.8663, val_loss=1.9203, val_mIoU=0.3259\n",
      "Epoch 88: train_loss=0.0831, train_mIoU=0.8752, val_loss=1.8539, val_mIoU=0.3245\n",
      "Epoch 89: train_loss=0.0795, train_mIoU=0.8681, val_loss=1.9529, val_mIoU=0.3260\n",
      "Epoch 90: train_loss=0.0814, train_mIoU=0.8814, val_loss=2.0086, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch090.pth\n",
      "Epoch 91: train_loss=0.0803, train_mIoU=0.8787, val_loss=2.0218, val_mIoU=0.3261\n",
      "Epoch 92: train_loss=0.0793, train_mIoU=0.8758, val_loss=2.1162, val_mIoU=0.3262\n",
      "Epoch 93: train_loss=0.0809, train_mIoU=0.8775, val_loss=2.0231, val_mIoU=0.3262\n",
      "Epoch 94: train_loss=0.0772, train_mIoU=0.8736, val_loss=2.0184, val_mIoU=0.3262\n",
      "Epoch 95: train_loss=0.0784, train_mIoU=0.8660, val_loss=2.0018, val_mIoU=0.3261\n",
      "Epoch 96: train_loss=0.0789, train_mIoU=0.8757, val_loss=2.0720, val_mIoU=0.3262\n",
      "Epoch 97: train_loss=0.0769, train_mIoU=0.8682, val_loss=2.0234, val_mIoU=0.3262\n",
      "Epoch 98: train_loss=0.0757, train_mIoU=0.8797, val_loss=2.1260, val_mIoU=0.3262\n",
      "Epoch 99: train_loss=0.0757, train_mIoU=0.8769, val_loss=2.1275, val_mIoU=0.3262\n",
      "Epoch 100: train_loss=0.0753, train_mIoU=0.8857, val_loss=2.1388, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch100.pth\n",
      "Epoch 101: train_loss=0.0744, train_mIoU=0.8775, val_loss=2.1837, val_mIoU=0.3262\n",
      "Epoch 102: train_loss=0.0742, train_mIoU=0.8778, val_loss=2.1008, val_mIoU=0.3262\n",
      "Epoch 103: train_loss=0.0735, train_mIoU=0.8835, val_loss=2.1614, val_mIoU=0.3262\n",
      "Epoch 104: train_loss=0.0741, train_mIoU=0.8874, val_loss=2.1642, val_mIoU=0.3262\n",
      "Epoch 105: train_loss=0.0741, train_mIoU=0.8790, val_loss=2.0504, val_mIoU=0.3262\n",
      "Epoch 106: train_loss=0.0720, train_mIoU=0.8750, val_loss=2.1095, val_mIoU=0.3262\n",
      "Epoch 107: train_loss=0.0715, train_mIoU=0.8847, val_loss=2.1074, val_mIoU=0.3262\n",
      "Epoch 108: train_loss=0.0736, train_mIoU=0.8853, val_loss=2.0911, val_mIoU=0.3262\n",
      "Epoch 109: train_loss=0.0729, train_mIoU=0.8828, val_loss=2.0580, val_mIoU=0.3254\n",
      "Epoch 110: train_loss=0.0709, train_mIoU=0.8777, val_loss=2.0824, val_mIoU=0.3259\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch110.pth\n",
      "Epoch 111: train_loss=0.0707, train_mIoU=0.8754, val_loss=2.1410, val_mIoU=0.3262\n",
      "Epoch 112: train_loss=0.0694, train_mIoU=0.8855, val_loss=2.1246, val_mIoU=0.3260\n",
      "Epoch 113: train_loss=0.0707, train_mIoU=0.8828, val_loss=2.1922, val_mIoU=0.3262\n",
      "Epoch 114: train_loss=0.0707, train_mIoU=0.8865, val_loss=2.2261, val_mIoU=0.3262\n",
      "Epoch 115: train_loss=0.0695, train_mIoU=0.8840, val_loss=2.2008, val_mIoU=0.3262\n",
      "Epoch 116: train_loss=0.0679, train_mIoU=0.8858, val_loss=2.1963, val_mIoU=0.3262\n",
      "Epoch 117: train_loss=0.0698, train_mIoU=0.8853, val_loss=2.1612, val_mIoU=0.3262\n",
      "Epoch 118: train_loss=0.0679, train_mIoU=0.8803, val_loss=2.2020, val_mIoU=0.3262\n",
      "Epoch 119: train_loss=0.0669, train_mIoU=0.8863, val_loss=2.1573, val_mIoU=0.3260\n",
      "Epoch 120: train_loss=0.0702, train_mIoU=0.8910, val_loss=2.0451, val_mIoU=0.3251\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch120.pth\n",
      "Epoch 121: train_loss=0.0685, train_mIoU=0.8860, val_loss=2.0663, val_mIoU=0.3258\n",
      "Epoch 122: train_loss=0.0674, train_mIoU=0.8822, val_loss=2.1898, val_mIoU=0.3262\n",
      "Epoch 123: train_loss=0.0655, train_mIoU=0.8826, val_loss=2.2943, val_mIoU=0.3262\n",
      "Epoch 124: train_loss=0.0679, train_mIoU=0.8842, val_loss=2.1764, val_mIoU=0.3262\n",
      "Epoch 125: train_loss=0.0675, train_mIoU=0.8959, val_loss=2.2228, val_mIoU=0.3262\n",
      "Epoch 126: train_loss=0.0647, train_mIoU=0.8845, val_loss=2.3138, val_mIoU=0.3262\n",
      "Epoch 127: train_loss=0.0660, train_mIoU=0.8836, val_loss=2.3037, val_mIoU=0.3262\n",
      "Epoch 128: train_loss=0.0656, train_mIoU=0.8919, val_loss=2.2610, val_mIoU=0.3262\n",
      "Epoch 129: train_loss=0.0653, train_mIoU=0.8889, val_loss=2.2428, val_mIoU=0.3262\n",
      "Epoch 130: train_loss=0.0659, train_mIoU=0.8889, val_loss=2.1205, val_mIoU=0.3260\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch130.pth\n",
      "Epoch 131: train_loss=0.0664, train_mIoU=0.8884, val_loss=2.1752, val_mIoU=0.3262\n",
      "Epoch 132: train_loss=0.0653, train_mIoU=0.8885, val_loss=2.2184, val_mIoU=0.3262\n",
      "Epoch 133: train_loss=0.0638, train_mIoU=0.8887, val_loss=2.2519, val_mIoU=0.3262\n",
      "Epoch 134: train_loss=0.0642, train_mIoU=0.8885, val_loss=2.2254, val_mIoU=0.3261\n",
      "Epoch 135: train_loss=0.0641, train_mIoU=0.8853, val_loss=2.3297, val_mIoU=0.3260\n",
      "Epoch 136: train_loss=0.0625, train_mIoU=0.8888, val_loss=2.3559, val_mIoU=0.3262\n",
      "Epoch 137: train_loss=0.0642, train_mIoU=0.8930, val_loss=2.3288, val_mIoU=0.3262\n",
      "Epoch 138: train_loss=0.0642, train_mIoU=0.8895, val_loss=2.3465, val_mIoU=0.3260\n",
      "Epoch 139: train_loss=0.0623, train_mIoU=0.8950, val_loss=2.3479, val_mIoU=0.3259\n",
      "Epoch 140: train_loss=0.0626, train_mIoU=0.8856, val_loss=2.3514, val_mIoU=0.3261\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch140.pth\n",
      "Epoch 141: train_loss=0.0615, train_mIoU=0.8942, val_loss=2.2523, val_mIoU=0.3250\n",
      "Epoch 142: train_loss=0.0612, train_mIoU=0.8922, val_loss=2.2582, val_mIoU=0.3254\n",
      "Epoch 143: train_loss=0.0613, train_mIoU=0.8849, val_loss=2.3095, val_mIoU=0.3257\n",
      "Epoch 144: train_loss=0.0614, train_mIoU=0.8947, val_loss=2.3217, val_mIoU=0.3262\n",
      "Epoch 145: train_loss=0.0620, train_mIoU=0.8908, val_loss=2.2506, val_mIoU=0.3262\n",
      "Epoch 146: train_loss=0.0599, train_mIoU=0.8931, val_loss=2.3315, val_mIoU=0.3262\n",
      "Epoch 147: train_loss=0.0601, train_mIoU=0.8962, val_loss=2.3777, val_mIoU=0.3262\n",
      "Epoch 148: train_loss=0.0592, train_mIoU=0.8908, val_loss=2.3825, val_mIoU=0.3262\n",
      "Epoch 149: train_loss=0.0601, train_mIoU=0.8898, val_loss=2.3962, val_mIoU=0.3262\n",
      "Epoch 150: train_loss=0.0610, train_mIoU=0.8891, val_loss=2.3130, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch150.pth\n",
      "Epoch 151: train_loss=0.0593, train_mIoU=0.8885, val_loss=2.4262, val_mIoU=0.3262\n",
      "Epoch 152: train_loss=0.0586, train_mIoU=0.8987, val_loss=2.4834, val_mIoU=0.3262\n",
      "Epoch 153: train_loss=0.0598, train_mIoU=0.9013, val_loss=2.4113, val_mIoU=0.3259\n",
      "Epoch 154: train_loss=0.0590, train_mIoU=0.8970, val_loss=2.3759, val_mIoU=0.3253\n",
      "Epoch 155: train_loss=0.0582, train_mIoU=0.8964, val_loss=2.3272, val_mIoU=0.3260\n",
      "Epoch 156: train_loss=0.0593, train_mIoU=0.8928, val_loss=2.3472, val_mIoU=0.3262\n",
      "Epoch 157: train_loss=0.0572, train_mIoU=0.8898, val_loss=2.4447, val_mIoU=0.3262\n",
      "Epoch 158: train_loss=0.0574, train_mIoU=0.8917, val_loss=2.4810, val_mIoU=0.3262\n",
      "Epoch 159: train_loss=0.0566, train_mIoU=0.8931, val_loss=2.4605, val_mIoU=0.3260\n",
      "Epoch 160: train_loss=0.0580, train_mIoU=0.8967, val_loss=2.4292, val_mIoU=0.3260\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch160.pth\n",
      "Epoch 161: train_loss=0.0579, train_mIoU=0.9018, val_loss=2.3795, val_mIoU=0.3262\n",
      "Epoch 162: train_loss=0.0573, train_mIoU=0.9005, val_loss=2.3467, val_mIoU=0.3262\n",
      "Epoch 163: train_loss=0.0564, train_mIoU=0.8969, val_loss=2.2727, val_mIoU=0.3259\n",
      "Epoch 164: train_loss=0.0564, train_mIoU=0.8873, val_loss=2.3949, val_mIoU=0.3260\n",
      "Epoch 165: train_loss=0.0576, train_mIoU=0.8918, val_loss=2.3434, val_mIoU=0.3253\n",
      "Epoch 166: train_loss=0.0559, train_mIoU=0.8916, val_loss=2.3482, val_mIoU=0.3260\n",
      "Epoch 167: train_loss=0.0556, train_mIoU=0.8985, val_loss=2.2950, val_mIoU=0.3260\n",
      "Epoch 168: train_loss=0.0560, train_mIoU=0.8979, val_loss=2.3561, val_mIoU=0.3259\n",
      "Epoch 169: train_loss=0.0560, train_mIoU=0.8968, val_loss=2.3100, val_mIoU=0.3260\n",
      "Epoch 170: train_loss=0.0551, train_mIoU=0.8993, val_loss=2.3364, val_mIoU=0.3259\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch170.pth\n",
      "Epoch 171: train_loss=0.0554, train_mIoU=0.8985, val_loss=2.3608, val_mIoU=0.3254\n",
      "Epoch 172: train_loss=0.0551, train_mIoU=0.8971, val_loss=2.3018, val_mIoU=0.3246\n",
      "Epoch 173: train_loss=0.0552, train_mIoU=0.8996, val_loss=2.3168, val_mIoU=0.3248\n",
      "Epoch 174: train_loss=0.0552, train_mIoU=0.8992, val_loss=2.3820, val_mIoU=0.3257\n",
      "Epoch 175: train_loss=0.0556, train_mIoU=0.9009, val_loss=2.3134, val_mIoU=0.3253\n",
      "Epoch 176: train_loss=0.0540, train_mIoU=0.8962, val_loss=2.2408, val_mIoU=0.3244\n",
      "Epoch 177: train_loss=0.0534, train_mIoU=0.8966, val_loss=2.2585, val_mIoU=0.3246\n",
      "Epoch 178: train_loss=0.0536, train_mIoU=0.8989, val_loss=2.3056, val_mIoU=0.3245\n",
      "Epoch 179: train_loss=0.0537, train_mIoU=0.8957, val_loss=2.3544, val_mIoU=0.3242\n",
      "Epoch 180: train_loss=0.0530, train_mIoU=0.9033, val_loss=2.3840, val_mIoU=0.3238\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch180.pth\n",
      "Epoch 181: train_loss=0.0541, train_mIoU=0.9000, val_loss=2.3763, val_mIoU=0.3247\n",
      "Epoch 182: train_loss=0.0540, train_mIoU=0.8957, val_loss=2.3816, val_mIoU=0.3248\n",
      "Epoch 183: train_loss=0.0534, train_mIoU=0.9017, val_loss=2.4707, val_mIoU=0.3250\n",
      "Epoch 184: train_loss=0.0535, train_mIoU=0.9021, val_loss=2.4834, val_mIoU=0.3254\n",
      "Epoch 185: train_loss=0.0529, train_mIoU=0.8961, val_loss=2.5001, val_mIoU=0.3261\n",
      "Epoch 186: train_loss=0.0524, train_mIoU=0.8990, val_loss=2.4327, val_mIoU=0.3260\n",
      "Epoch 187: train_loss=0.0523, train_mIoU=0.8986, val_loss=2.4558, val_mIoU=0.3262\n",
      "Epoch 188: train_loss=0.0518, train_mIoU=0.9065, val_loss=2.5310, val_mIoU=0.3261\n",
      "Epoch 189: train_loss=0.0518, train_mIoU=0.9010, val_loss=2.5780, val_mIoU=0.3261\n",
      "Epoch 190: train_loss=0.0513, train_mIoU=0.8997, val_loss=2.5816, val_mIoU=0.3262\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch190.pth\n",
      "Epoch 191: train_loss=0.0516, train_mIoU=0.8956, val_loss=2.5835, val_mIoU=0.3260\n",
      "Epoch 192: train_loss=0.0518, train_mIoU=0.9016, val_loss=2.5648, val_mIoU=0.3260\n",
      "Epoch 193: train_loss=0.0513, train_mIoU=0.9012, val_loss=2.5409, val_mIoU=0.3252\n",
      "Epoch 194: train_loss=0.0518, train_mIoU=0.9013, val_loss=2.4555, val_mIoU=0.3251\n",
      "Epoch 195: train_loss=0.0512, train_mIoU=0.9041, val_loss=2.4709, val_mIoU=0.3250\n",
      "Epoch 196: train_loss=0.0524, train_mIoU=0.9025, val_loss=2.4931, val_mIoU=0.3248\n",
      "Epoch 197: train_loss=0.0521, train_mIoU=0.9029, val_loss=2.4801, val_mIoU=0.3243\n",
      "Epoch 198: train_loss=0.0509, train_mIoU=0.8980, val_loss=2.4772, val_mIoU=0.3241\n",
      "Epoch 199: train_loss=0.0516, train_mIoU=0.9054, val_loss=2.4850, val_mIoU=0.3245\n",
      "Epoch 200: train_loss=0.0508, train_mIoU=0.9003, val_loss=2.6652, val_mIoU=0.3256\n",
      "Saving Model...\n",
      "Model saved to C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch200.pth\n",
      "Training complete!\n",
      "-----EVALUATION PHASE-----\n",
      "Evaluating saved epoch models on test set...\n",
      "Model 20260103_1810_epoch010.pth -> test_loss=0.6044, test_mIoU=0.3569\n",
      "Model 20260103_1810_epoch020.pth -> test_loss=0.6579, test_mIoU=0.5690\n",
      "Model 20260103_1810_epoch030.pth -> test_loss=0.8348, test_mIoU=0.5180\n",
      "Model 20260103_1810_epoch040.pth -> test_loss=0.8445, test_mIoU=0.5964\n",
      "Model 20260103_1810_epoch050.pth -> test_loss=0.8964, test_mIoU=0.5409\n",
      "Model 20260103_1810_epoch060.pth -> test_loss=1.0715, test_mIoU=0.4201\n",
      "Model 20260103_1810_epoch070.pth -> test_loss=1.1492, test_mIoU=0.4201\n",
      "Model 20260103_1810_epoch080.pth -> test_loss=1.1489, test_mIoU=0.4201\n",
      "Model 20260103_1810_epoch090.pth -> test_loss=1.2029, test_mIoU=0.4296\n",
      "Model 20260103_1810_epoch100.pth -> test_loss=1.1855, test_mIoU=0.4479\n",
      "Model 20260103_1810_epoch110.pth -> test_loss=1.2977, test_mIoU=0.4201\n",
      "Model 20260103_1810_epoch120.pth -> test_loss=1.3222, test_mIoU=0.4201\n",
      "Model 20260103_1810_epoch130.pth -> test_loss=1.2971, test_mIoU=0.4292\n",
      "Model 20260103_1810_epoch140.pth -> test_loss=1.2320, test_mIoU=0.5249\n",
      "Model 20260103_1810_epoch150.pth -> test_loss=1.2196, test_mIoU=0.5370\n",
      "Model 20260103_1810_epoch160.pth -> test_loss=1.3691, test_mIoU=0.4422\n",
      "Model 20260103_1810_epoch170.pth -> test_loss=1.2151, test_mIoU=0.5529\n",
      "Model 20260103_1810_epoch180.pth -> test_loss=1.4017, test_mIoU=0.4722\n",
      "Model 20260103_1810_epoch190.pth -> test_loss=1.3449, test_mIoU=0.5080\n",
      "Model 20260103_1810_epoch200.pth -> test_loss=1.4035, test_mIoU=0.5239\n",
      "Best model: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch040.pth with test_mIoU=0.5964\n",
      "Ended at 20260103_18:11:13\n",
      "-----TRAINING AND EVALUATION ALL DONE!!!-----\n",
      "Outputs:\n",
      " - models: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\n",
      " - per-epoch test CSV: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Result_Segmentation\\\\20260103_1810\\per_10epoch_test_results_20260103_1810.csv\n",
      " - best model: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Weights\\\\20260103_1810\\20260103_1810_epoch040.pth\n",
      " - per-image CSV: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Result_Segmentation\\\\20260103_1810\\Bestmodel_epoch040_per_image_metrics_20260103_1810.csv\n",
      " - visualizations: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Result_Segmentation\\\\20260103_1810\\Visualizations\n",
      " - history CSV: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\History\\\\20260103_1810\\history_20260103_1810.csv\n",
      " - plots: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\Result_Segmentation\\\\20260103_1810\\plots\n",
      " - run info: C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\\History\\\\20260103_1810\\runinfo_20260103_1810.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unified DeepLabV3+ training / validation / testing pipeline\n",
    "Supports: CE (with class weights), Dice, Focal losses\n",
    "- Computes class weights from train masks\n",
    "- Saves per-epoch model files (filename contains filedate_epochXX.pth)\n",
    "- Evaluates all saved epoch models on test set and selects best by test mIoU\n",
    "- Computes detailed metrics for best model and per-image CSV\n",
    "- Generates side-by-side visualizations for each test image\n",
    "- Saves training history (CSV) and plots\n",
    "- Records run time to TXT\n",
    "\n",
    "Requirements:\n",
    "- torch\n",
    "- torchvision (ensure it includes deeplabv3_resnet101)\n",
    "- PIL, numpy, pandas, matplotlib\n",
    "\n",
    "Place your images and masks in folders and set `train_img_dir`, `train_mask_dir`, etc., below or pass via command-line \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "\n",
    "print(\"Library import successful.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration (default)\n",
    "# -----------------------------\n",
    "IMG_SIZE = (64, 64)  # (H, W)\n",
    "Channels = 3\n",
    "batch_size = 2\n",
    "classes = ['background', 'debris']\n",
    "num_classes = len(classes)\n",
    "num_epochs = 200 #10以上にしないと重みセーブできなくてエラーになる\n",
    "valsplit = 0.25\n",
    "learning_rate = 1e-4\n",
    "loss_type = \"ce\"  # \"ce\" / \"dice\" / \"focal\"\n",
    "optimizer_type = \"adam\"  # adam / sgd\n",
    "gamma_focal = 2.0\n",
    "alpha_focal = None  # list or None, set after class weights computed\n",
    "\n",
    "# Data paths (set to your dataset)\n",
    "root_dir = r\"C:\\Users\\kyohe\\Aerial_Photo_Segmenter\\20251209Data\"\n",
    "\n",
    "# Input dirs: the img and its mask has to have THE SAME FILENAME (different extensions allowed), or else they won't be paired.\n",
    "train_img_dir = Path(root_dir + r\"\\TrainVal\\img\")\n",
    "train_mask_dir = root_dir + r\"\\TrainVal\\mask\"\n",
    "\n",
    "val_img_dir = None  # if None, split from train\n",
    "val_mask_dir = None\n",
    "test_img_dir = root_dir + r\"\\Test\\img\"\n",
    "test_mask_dir = root_dir + r\"\\Test\\mask\"\n",
    "\n",
    "# Output dirs (will include filedate)\n",
    "history_root = root_dir + r\"\\History\\\\\"\n",
    "model_root = root_dir + r\"\\Weights\\\\\"\n",
    "result_root = root_dir + r\"\\Result_Segmentation\\\\\"\n",
    "os.makedirs(history_root, exist_ok=True)\n",
    "os.makedirs(result_root, exist_ok=True)\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "\n",
    "filedate = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "history_dir = history_root + filedate\n",
    "model_dir = model_root + filedate\n",
    "result_dir = result_root + filedate\n",
    "viz_dir = result_dir + r\"\\Visualizations\"\n",
    "pred_dir = result_dir + r\"\\PredMasks\"\n",
    "\n",
    "os.makedirs(history_dir, exist_ok=True)\n",
    "history_csv = os.path.join(history_dir, f\"history_{filedate}.csv\")\n",
    "runinfo_txt = os.path.join(history_dir, f\"runinfo_{filedate}.txt\")\n",
    "plots_dir = os.path.join(result_dir, \"plots\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"IMG_SIZE: {IMG_SIZE}, batch_size: {batch_size}, classes: {classes}, num_classes: {num_classes}\",\n",
    "      f\"\\nnum_epochs: {num_epochs}, valsplit: {valsplit}, learning_rate: {learning_rate}\",\n",
    "      f\"\\nloss_type: {loss_type}, optimizer_type: {optimizer_type}, DEVICE: {DEVICE}\",\n",
    "      \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_paths, img_size=IMG_SIZE, transforms=None):\n",
    "        assert len(img_paths) == len(mask_paths)\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.img_size = img_size\n",
    "        # transforms for image (tensor & normalize)\n",
    "        self.transforms = transforms or T.Compose([\n",
    "            T.Resize(img_size, interpolation=Image.BILINEAR),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # mask transforms (nearest, preserve labels)\n",
    "        self.mask_transform = T.Compose([\n",
    "            T.Resize(img_size, interpolation=Image.NEAREST),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        img = self.transforms(img)\n",
    "        mask = self.mask_transform(mask)\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "        # Convert {0,255} -> {0,1}\n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return img, mask, os.path.basename(self.img_paths[idx])\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: file pairing\n",
    "# -----------------------------\n",
    "\n",
    "def pair_images_and_masks(img_dir, mask_dir, img_exts=['.jpg', '.png', '.tif', '.tiff'], mask_exts=['.png']):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    for ext in img_exts:\n",
    "        imgs.extend(glob.glob(os.path.join(img_dir, f\"*{ext}\")))\n",
    "    imgs = sorted(imgs)\n",
    "    mask_map = {}\n",
    "    for ext in mask_exts:\n",
    "        for p in glob.glob(os.path.join(mask_dir, f\"*{ext}\")):\n",
    "            mask_map[os.path.splitext(os.path.basename(p))[0]] = p\n",
    "    img_paths = []\n",
    "    mask_paths = []\n",
    "    for p in imgs:\n",
    "        stem = os.path.splitext(os.path.basename(p))[0]\n",
    "        if stem in mask_map:\n",
    "            img_paths.append(p)\n",
    "            mask_paths.append(mask_map[stem])\n",
    "    return img_paths, mask_paths\n",
    "\n",
    "# For the Test Dataset: record original sizes\n",
    "def pair_test_images_and_masks(img_dir, mask_dir, img_exts=['.jpg', '.png', '.tif', '.tiff'], mask_exts=['.png']):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    for ext in img_exts:\n",
    "        imgs.extend(glob.glob(os.path.join(img_dir, f\"*{ext}\")))\n",
    "    imgs = sorted(imgs)\n",
    "    mask_map = {}\n",
    "    for ext in mask_exts:\n",
    "        for p in glob.glob(os.path.join(mask_dir, f\"*{ext}\")):\n",
    "            mask_map[os.path.splitext(os.path.basename(p))[0]] = p\n",
    "            \n",
    "    img_paths = []\n",
    "    mask_paths = []\n",
    "    test_orig_sizes = {}\n",
    "    for p in imgs:\n",
    "        stem = os.path.splitext(os.path.basename(p))[0]\n",
    "        if stem in mask_map:\n",
    "            img_paths.append(p)\n",
    "            mask_paths.append(mask_map[stem])\n",
    "            try:\n",
    "                with Image.open(p) as im:\n",
    "                    w, h = im.size\n",
    "                test_orig_sizes[stem] = (h, w)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return img_paths, mask_paths, test_orig_sizes\n",
    "\n",
    "# -----------------------------\n",
    "# Class weight computation\n",
    "# -----------------------------\n",
    "\n",
    "def compute_class_weights(dataset):\n",
    "    counts = np.zeros(num_classes, dtype=np.int64)\n",
    "    for i in range(len(dataset)):\n",
    "        _, mask, _ = dataset[i]\n",
    "        mask_np = mask.numpy().ravel()\n",
    "        for c in range(num_classes):\n",
    "            counts[c] += int((mask_np == c).sum())\n",
    "    total = counts.sum()\n",
    "    freq = counts / total\n",
    "    # weight: inverse of frequency\n",
    "    weights = total / (num_classes * counts)\n",
    "    weights = weights.astype(np.float32)\n",
    "    return weights, counts, freq\n",
    "\n",
    "# -----------------------------\n",
    "# Losses\n",
    "# -----------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits: [B,C,H,W], target: [B,H,W]\n",
    "        # num_classes = logits.shape[1]\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        target_onehot = nn.functional.one_hot(target, num_classes).permute(0,3,1,2).float()\n",
    "        dims = (0,2,3)\n",
    "        intersection = torch.sum(probs * target_onehot, dims)\n",
    "        cardinality = torch.sum(probs + target_onehot, dims)\n",
    "        dice_score = (2. * intersection + self.eps) / (cardinality + self.eps)\n",
    "        loss = 1. - dice_score.mean()\n",
    "        return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # logits: [B,C,H,W], target: [B,H,W]\n",
    "        ce = nn.functional.cross_entropy(logits, target, reduction='none')\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pt = probs.gather(1, target.unsqueeze(1)).squeeze(1)  # [B,H,W]\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        loss = focal_term * ce\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            at = alpha.gather(0, target.flatten()).view_as(target).to(logits.device)\n",
    "            loss = at * loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "\n",
    "def confusion_matrix_from_logits(logits, target):\n",
    "    # logits: [B,C,H,W] or [B,1,H,W]\n",
    "    preds = torch.argmax(logits, dim=1).view(-1).cpu().numpy()\n",
    "    t = target.view(-1).cpu().numpy()\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    for gt, pd in zip(t, preds):\n",
    "        cm[gt, pd] += 1\n",
    "    return cm\n",
    "\n",
    "\n",
    "def compute_metrics_from_cm(cm):\n",
    "    # cm: [num_classes,num_classes] where rows=gt, cols=pred\n",
    "    num_classes = cm.shape[0]\n",
    "    eps = 1e-6\n",
    "    tp = np.diag(cm).astype(float)\n",
    "    fp = np.sum(cm, axis=0) - tp\n",
    "    fn = np.sum(cm, axis=1) - tp\n",
    "    tn = cm.sum() - (tp + fp + fn)\n",
    "    # per-class IoU\n",
    "    iou = tp / (tp + fp + fn + eps)\n",
    "    mean_iou = np.nanmean(iou)\n",
    "    pixel_acc = tp.sum() / (cm.sum() + eps)\n",
    "    # per-class accuracy: tp / (tp + fn)\n",
    "    class_acc = tp / (tp + fn + eps)\n",
    "    mean_acc = np.nanmean(class_acc)\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return {\n",
    "        'iou_per_class': iou,\n",
    "        'mean_iou': mean_iou,\n",
    "        'pixel_acc': pixel_acc,\n",
    "        'mean_acc': mean_acc,\n",
    "        'precision_per_class': precision,\n",
    "        'recall_per_class': recall,\n",
    "        'f1_per_class': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Eval loops\n",
    "# -----------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_cm = np.zeros((2,2), dtype=np.int64)\n",
    "    n = 0\n",
    "    for imgs, masks, _ in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)['out'] if isinstance(model(imgs), dict) else model(imgs)\n",
    "        loss = criterion(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        cm = confusion_matrix_from_logits(logits.detach(), masks.detach())\n",
    "        running_cm += cm\n",
    "        n += imgs.size(0)\n",
    "    avg_loss = running_loss / n\n",
    "    metrics = compute_metrics_from_cm(running_cm)\n",
    "    return avg_loss, metrics['mean_iou']\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_cm = np.zeros((2,2), dtype=np.int64)\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(imgs)['out'] if isinstance(model(imgs), dict) else model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            cm = confusion_matrix_from_logits(logits, masks)\n",
    "            running_cm += cm\n",
    "            n += imgs.size(0)\n",
    "    avg_loss = running_loss / n\n",
    "    metrics = compute_metrics_from_cm(running_cm)\n",
    "    return avg_loss, metrics['mean_iou']\n",
    "\n",
    "# -----------------------------\n",
    "# Full evaluation utilities\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate_model_on_test(model, testset, criterion, device):\n",
    "    # As requested, use batch_size = len(testset)\n",
    "    testloader = DataLoader(testset, batch_size=len(testset))\n",
    "    return eval_one_epoch(model, testloader, criterion, device)\n",
    "\n",
    "\n",
    "def detailed_test_evaluation(model, testset, criterion, device, viz_dir=None, per_image_csv=None, overall_image_csv=None, test_orig_sizes=None):\n",
    "    \"\"\"Evaluate overall metrics and per-image metrics + save visualizations.\n",
    "\n",
    "    Adds per-image TP/FP/TN/FN/Total pixel counts (binary positive=class 1),\n",
    "    and per-class IoU values to the per-image CSV.\n",
    "    Also appends aggregate binary counts to the returned overall metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    per_image_rows = []\n",
    "    testloader = DataLoader(testset, batch_size=1, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks, names in testloader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(imgs)['out'] if isinstance(model(imgs), dict) else model(imgs)\n",
    "            loss = criterion(logits, masks).item()\n",
    "            cm = confusion_matrix_from_logits(logits, masks)\n",
    "            all_cm += cm\n",
    "            m = compute_metrics_from_cm(cm)\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "            gt = masks.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            # Save visualization (if requested)\n",
    "            if viz_dir is not None:\n",
    "                save_visualization(imgs.squeeze(0).cpu(), gt, pred, names[0], viz_dir, pred_dir, test_orig_sizes=test_orig_sizes)\n",
    "\n",
    "            # Binary confusion counts for class 1 (debris) vs class 0 (background)\n",
    "            # For 2-class case: TP = cm[1,1], FP = cm[0,1], FN = cm[1,0], TN = cm[0,0]\n",
    "            total_pixels = int(cm.sum())\n",
    "\n",
    "            # per-class IoU\n",
    "            iou_c = m['iou_per_class']\n",
    "            iou_c0 = float(iou_c[0])\n",
    "            iou_c1 = float(iou_c[1])\n",
    "\n",
    "            row = {\n",
    "                'image': names[0],\n",
    "                'loss': loss,\n",
    "                'mIoU': m['mean_iou'],\n",
    "                'pixel_acc': m['pixel_acc'],\n",
    "                'mean_acc': m['mean_acc'],\n",
    "                'iou_class0': iou_c0,\n",
    "                'iou_class1': iou_c1,\n",
    "                'precision_class0': m['precision_per_class'][0],\n",
    "                'precision_class1': m['precision_per_class'][1],\n",
    "                'recall_class0': m['recall_per_class'][0],\n",
    "                'recall_class1': m['recall_per_class'][1],\n",
    "                'f1_class0': m['f1_per_class'][0],\n",
    "                'f1_class1': m['f1_per_class'][1],\n",
    "                'TP': m['tp'],\n",
    "                'FP': m['fp'],\n",
    "                'TN': m['tn'],\n",
    "                'FN': m['fn'],\n",
    "                'Total': total_pixels,\n",
    "            }\n",
    "            per_image_rows.append(row)\n",
    "    \n",
    "    if per_image_csv is not None:\n",
    "        df = pd.DataFrame(per_image_rows)\n",
    "        df.to_csv(per_image_csv, index=False)\n",
    "\n",
    "    overall = compute_metrics_from_cm(all_cm)\n",
    "    # Add aggregate binary counts to the overall metrics for convenience\n",
    "\n",
    "    overall_row = {\n",
    "        'pixel_acc': overall['pixel_acc'],\n",
    "        'mean_acc': overall['mean_acc'],\n",
    "        'iou_class0': float(overall['iou_per_class'][0]),\n",
    "        'iou_class1': float(overall['iou_per_class'][1]),\n",
    "        'precision_class0': overall['precision_per_class'][0],\n",
    "        'precision_class1': overall['precision_per_class'][1],\n",
    "        'recall_class0': overall['recall_per_class'][0],\n",
    "        'recall_class1': overall['recall_per_class'][1],\n",
    "        'f1_class0': overall['f1_per_class'][0],\n",
    "        'f1_class1': overall['f1_per_class'][1],\n",
    "        'TP': overall['tp'],\n",
    "        'FP': overall['fp'],\n",
    "        'TN': overall['tn'],\n",
    "        'FN': overall['fn'],\n",
    "        'Total': int(all_cm.sum()),\n",
    "    }\n",
    "\n",
    "    if overall_image_csv is not None:\n",
    "        df = pd.DataFrame([overall_row])\n",
    "        df.to_csv(overall_image_csv, index=False)\n",
    "\n",
    "    return overall_row, per_image_rows\n",
    "\n",
    "\n",
    "def save_visualization(img_tensor, gt_mask, pred_mask, name, viz_dir, pred_dir, test_orig_sizes={}):\n",
    "    \"\"\"Save side-by-side visualization (Input | GT | Pred) and also save the predicted mask\n",
    "    as a separate grayscale PNG under viz_dir/pred_masks/ with filename <name>_pred_mask.png.\n",
    "    \"\"\"\n",
    "    # img_tensor: normalized tensor; convert back to RGB\n",
    "    img = img_tensor.clone()\n",
    "    img = img * torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n",
    "    img = img + torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
    "    img = img.clamp(0,1).permute(1,2,0).numpy()\n",
    "    gt = gt_mask\n",
    "    pred = pred_mask\n",
    "\n",
    "    # Stack horizontally\n",
    "    fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(gt, cmap='gray')\n",
    "    axes[1].set_title('GT')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].imshow(pred, cmap='gray')\n",
    "    axes[2].set_title('Pred')\n",
    "    axes[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Ensure visualization dir exists and save combined image\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    outpath = os.path.join(viz_dir, f\"{os.path.splitext(name)[0]}_viz.png\")\n",
    "    plt.savefig(outpath, dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Also save predicted mask as a grayscale PNG (0 or 255 values) in 'pred_masks' subdir\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    # pred may be 0/1; convert to 0/255 uint8\n",
    "    pred_img = (pred.astype(np.uint8) * 255)\n",
    "    pred_pil = Image.fromarray(pred_img, mode='L')\n",
    "    # Determine orig size: precedence argument -> test_orig_sizes mapping\n",
    "    stem = os.path.splitext(name)[0]\n",
    "    if test_orig_sizes is not None:\n",
    "        orig = test_orig_sizes.get(stem, None)\n",
    "        # orig is (H, W)\n",
    "        orig_w = int(orig[1])\n",
    "        orig_h = int(orig[0])\n",
    "        pred_pil = pred_pil.resize((orig_w, orig_h), resample=Image.NEAREST)\n",
    "    pred_out = os.path.join(pred_dir, f\"{stem}_pred_mask.png\")\n",
    "    pred_pil.save(pred_out)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot helpers\n",
    "# -----------------------------\n",
    "\n",
    "def plot_loss(history_csv, outpath):\n",
    "    df = pd.read_csv(history_csv)\n",
    "    plt.figure()\n",
    "    plt.plot(df['epoch'], df['train_loss'], label='train_loss')\n",
    "    plt.plot(df['epoch'], df['val_loss'], label='val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_acc(history_csv, outpath):\n",
    "    df = pd.read_csv(history_csv)\n",
    "    plt.figure()\n",
    "    plt.plot(df['epoch'], df['train_mIoU'], label='train_mIoU')\n",
    "    plt.plot(df['epoch'], df['val_mIoU'], label='val_mIoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('mIoU')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(outpath)\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# Main pipeline\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    start_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "    print(f\"Started at {start_time}\")\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_imgs, train_masks = pair_images_and_masks(train_img_dir, train_mask_dir)\n",
    "    test_imgs, test_masks, test_orig_sizes = pair_test_images_and_masks(test_img_dir, test_mask_dir)\n",
    "    assert len(train_imgs) > 0, 'No training images found'\n",
    "    assert len(test_imgs) > 0, 'No test images found'\n",
    "\n",
    "    # Split train/val if val dirs not provided\n",
    "    if val_img_dir is None or val_mask_dir is None:\n",
    "        n = len(train_imgs)\n",
    "        nval = max(1, int(n * valsplit))\n",
    "        # simple split\n",
    "        val_imgs = train_imgs[:nval]\n",
    "        val_masks = train_masks[:nval]\n",
    "        train_imgs2 = train_imgs[nval:]\n",
    "        train_masks2 = train_masks[nval:]\n",
    "    else:\n",
    "        val_imgs, val_masks = pair_images_and_masks(val_img_dir, val_mask_dir)\n",
    "        train_imgs2, train_masks2 = train_imgs, train_masks\n",
    "\n",
    "    trainset = MyDataset(train_imgs2, train_masks2, img_size=IMG_SIZE)\n",
    "    valset = MyDataset(val_imgs, val_masks, img_size=IMG_SIZE)\n",
    "    testset = MyDataset(test_imgs, test_masks, img_size=IMG_SIZE)\n",
    "\n",
    "    # Dataloaders\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valloader = DataLoader(valset, batch_size=min(len(valset), batch_size), shuffle=False, num_workers=0)\n",
    "\n",
    "    size = len(trainloader.dataset)\n",
    "    num_batches = len(trainloader)\n",
    "    size_val = len(valloader.dataset)\n",
    "    num_batches_val = len(valloader)\n",
    "    size_test = len(testset)\n",
    "    print(f\"TrainData Size: {size}, TrainData Batches: {num_batches}\", \n",
    "          f\"\\nValData Size: {size_val}, ValData Batches: {num_batches_val}\"\n",
    "          f\"\\nTestData Size: {size_test}, TestData Batches: 1\")\n",
    "\n",
    "    # Class weights\n",
    "    class_weights, counts, freq = compute_class_weights(trainset)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "    print(f\"Class counts: {counts}, freq: {freq}, weights: {class_weights}\")\n",
    "\n",
    "    # Prepare model (use torchvision's deeplabv3_resnet101)\n",
    "    model = deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.DEFAULT)\n",
    "    # ヘッドを置き換え\n",
    "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Loss selection\n",
    "    if loss_type == 'ce':\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    elif loss_type == 'dice':\n",
    "        criterion = DiceLoss()\n",
    "    elif loss_type == 'focal':\n",
    "        alpha = alpha_focal if alpha_focal is not None else class_weights.cpu().numpy().tolist()\n",
    "        criterion = FocalLoss(gamma=gamma_focal, alpha=alpha)\n",
    "    else:\n",
    "        raise ValueError('Unknown loss type')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_type.lower() == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # History store\n",
    "    history = []\n",
    "\n",
    "    # Training loop\n",
    "    print(\"-----TRAINING PHASE-----\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_miou = train_one_epoch(model, trainloader, optimizer, criterion, DEVICE)\n",
    "        val_loss, val_miou = eval_one_epoch(model, valloader, criterion, DEVICE)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_mIoU={train_miou:.4f}, val_loss={val_loss:.4f}, val_mIoU={val_miou:.4f}\")\n",
    "        history.append({'epoch': epoch, 'train_loss': train_loss, 'train_mIoU': train_miou, 'val_loss': val_loss, 'val_mIoU': val_miou})\n",
    "\n",
    "        # Save every 10 epoch (to support per10-epoch test evaluation).\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Saving Model...\")\n",
    "            model_path = os.path.join(model_dir, f\"{filedate}_epoch{epoch:03d}.pth\")\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"Model saved to\", model_path)\n",
    "\n",
    "        # Save history to CSV each epoch\n",
    "        df = pd.DataFrame(history)\n",
    "        df.to_csv(history_csv, index=False)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    print(\"-----EVALUATION PHASE-----\")\n",
    "\n",
    "    # After training: evaluate all saved epoch models on test set\n",
    "    print('Evaluating saved epoch models on test set...')\n",
    "    model_files = sorted(glob.glob(os.path.join(model_dir, f\"{filedate}_epoch*.pth\")))\n",
    "    per_epoch_results = []\n",
    "    for p in model_files:\n",
    "        state = torch.load(p, map_location=DEVICE)\n",
    "        model.load_state_dict(state)\n",
    "        test_loss, test_miou = evaluate_model_on_test(model, testset, criterion, DEVICE)\n",
    "        per_epoch_results.append({'model_file': os.path.basename(p), 'test_loss': test_loss, 'test_mIoU': test_miou})\n",
    "        print(f\"Model {os.path.basename(p)} -> test_loss={test_loss:.4f}, test_mIoU={test_miou:.4f}\")\n",
    "\n",
    "    per_epoch_df = pd.DataFrame(per_epoch_results)\n",
    "    per_epoch_csv = os.path.join(result_dir, f\"per_10epoch_test_results_{filedate}.csv\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    per_epoch_df.to_csv(per_epoch_csv, index=False)\n",
    "\n",
    "    # Select best model by test mIoU\n",
    "    best_row = per_epoch_df.loc[per_epoch_df['test_mIoU'].idxmax()]\n",
    "    best_model_file = os.path.join(model_dir, best_row['model_file'])\n",
    "    print(f\"Best model: {best_model_file} with test_mIoU={best_row['test_mIoU']:.4f}\")\n",
    "    best_epoch = best_row['model_file'].split('_')[-1].split('.')[0]\n",
    "    state = torch.load(best_model_file, map_location=DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    # Detailed evaluation with best model\n",
    "    per_image_csv = os.path.join(result_dir, f\"Bestmodel_{best_epoch}_per_image_metrics_{filedate}.csv\")\n",
    "    overall_image_csv = os.path.join(result_dir, f\"Bestmodel_{best_epoch}_overall_image_metrics_{filedate}.csv\")\n",
    "    overall_metrics, per_image_rows = detailed_test_evaluation(\n",
    "        model, testset, criterion, DEVICE, viz_dir=viz_dir, per_image_csv=per_image_csv, overall_image_csv=overall_image_csv, test_orig_sizes=test_orig_sizes)\n",
    "\n",
    "    # Plots\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    plot_loss(history_csv, os.path.join(plots_dir, f\"loss_{filedate}.png\"))\n",
    "    plot_acc(history_csv, os.path.join(plots_dir, f\"acc_{filedate}.png\"))\n",
    "\n",
    "    end_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "    print(f\"Ended at {end_time}\")\n",
    "\n",
    "    with open(runinfo_txt, 'w') as f:\n",
    "        f.write(f\"Start: {start_time}\\n\")\n",
    "        f.write(f\"End: {end_time}\\n\")\n",
    "\n",
    "    print('-----TRAINING AND EVALUATION ALL DONE!!!-----')\n",
    "    print('Outputs:')\n",
    "    print(f\" - models: {model_dir}\")\n",
    "    print(f\" - per-epoch test CSV: {per_epoch_csv}\")\n",
    "    print(f\" - best model: {best_model_file}\")\n",
    "    print(f\" - per-image CSV: {per_image_csv}\")\n",
    "    print(f\" - visualizations: {viz_dir}\")\n",
    "    print(f\" - history CSV: {history_csv}\")\n",
    "    print(f\" - plots: {plots_dir}\")\n",
    "    print(f\" - run info: {runinfo_txt}\")\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aerial_Photo_Segmenter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
